{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeakinwe/computer_vision/blob/main/SqueezeNet_PyTorch_5_flowers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekVzSW8Bb_Ze",
        "outputId": "da1f87d3-4b47-491c-b6bf-802f019965dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import wandb\n",
        "import os"
      ],
      "metadata": {
        "id": "HUjjD6MRYJmr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 0: Initialize wandb\n",
        "# =======================\n",
        "wandb.init(project=\"Squeezenet-5flowers-Classifier\", config={\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"SqueezeNet\",\n",
        "    \"pretrained\": True,\n",
        "    \"input_size\": 224\n",
        "})\n",
        "\n",
        "# Shortcut to config values\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "zxVl69yfYLO3",
        "outputId": "420d0767-003d-411b-e902-eb66168cf502"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madeakinwe\u001b[0m (\u001b[33madeakinwe-richkinwe-io\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250713_120054-94crp4qm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/adeakinwe-richkinwe-io/Squeezenet-5flowers-Classifier/runs/94crp4qm' target=\"_blank\">major-smoke-1</a></strong> to <a href='https://wandb.ai/adeakinwe-richkinwe-io/Squeezenet-5flowers-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/adeakinwe-richkinwe-io/Squeezenet-5flowers-Classifier' target=\"_blank\">https://wandb.ai/adeakinwe-richkinwe-io/Squeezenet-5flowers-Classifier</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/adeakinwe-richkinwe-io/Squeezenet-5flowers-Classifier/runs/94crp4qm' target=\"_blank\">https://wandb.ai/adeakinwe-richkinwe-io/Squeezenet-5flowers-Classifier/runs/94crp4qm</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 1: Data Preparation\n",
        "# =======================\n",
        "\n",
        "# Transforms for training and validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/5flowersdata/flowers/train\"\n",
        "val_dir = \"/content/drive/MyDrive/5flowersdata/flowers/val\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)"
      ],
      "metadata": {
        "id": "Ti4rs4tkYMos"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# STEP 2: Load Pretrained Model\n",
        "# ===========================\n",
        "from torchvision.models import SqueezeNet1_1_Weights\n",
        "\n",
        "model = models.squeezenet1_1(weights=SqueezeNet1_1_Weights.DEFAULT)\n",
        "\n",
        "# Change the final conv layer to match 5 flower classes\n",
        "model.classifier[1] = nn.Conv2d(in_channels=512, out_channels=5, kernel_size=1)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze only final conv layer\n",
        "for param in model.classifier[1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Watch the model's weights and gradients\n",
        "wandb.watch(model, log=\"all\", log_freq=10)"
      ],
      "metadata": {
        "id": "Uu1_CDgvYOEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555aedd5-8b13-4dc3-c149-ce74da99271b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n",
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 31.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# STEP 3: Loss & Optimizer\n",
        "# ===================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "id": "mxiVbXfxYPiW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_g-opvZQESck"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            batch_correct = (preds == labels).sum().item()\n",
        "            train_correct += batch_correct\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            # Print every 10 batches\n",
        "            if (i + 1) % 10 == 0:\n",
        "                batch_acc = batch_correct / labels.size(0)\n",
        "                print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
        "\n",
        "        train_acc = train_correct / train_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": running_loss, \"train_accuracy\": train_acc})\n",
        "        print(f\"Epoch {epoch+1} Summary - Loss: {running_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"val_accuracy\": val_acc})\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# Train the model\n",
        "# ===================\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=config.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTczSawNYSrI",
        "outputId": "14275976-c89f-458e-99af-b3f321c72fdd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 1.9919, Batch Acc: 0.3125\n",
            "[Batch 20/251] Loss: 1.4227, Batch Acc: 0.5000\n",
            "[Batch 30/251] Loss: 1.2338, Batch Acc: 0.4375\n",
            "[Batch 40/251] Loss: 1.0337, Batch Acc: 0.6250\n",
            "[Batch 50/251] Loss: 1.0015, Batch Acc: 0.6250\n",
            "[Batch 60/251] Loss: 0.6974, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.6616, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.6633, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.8809, Batch Acc: 0.6250\n",
            "[Batch 100/251] Loss: 0.4755, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.5008, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.4186, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.4068, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.5251, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.3216, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.4963, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.7981, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.8824, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.3812, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.4574, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.1477, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.7282, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.4422, Batch Acc: 0.7500\n",
            "[Batch 240/251] Loss: 0.2864, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.1500, Batch Acc: 0.9375\n",
            "Epoch 1 Summary - Loss: 179.7591, Train Accuracy: 0.7358\n",
            "Validation Accuracy: 0.8437\n",
            "\n",
            "Epoch 2/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1913, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.4647, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.3050, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.3015, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.4724, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.9182, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 0.1843, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.2226, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.4124, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.5676, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.2098, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.0919, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.5142, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.5397, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.4826, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.3927, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.3138, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.5423, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.4103, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.5273, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.5809, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.3859, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.6850, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.1868, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1342, Batch Acc: 1.0000\n",
            "Epoch 2 Summary - Loss: 98.4382, Train Accuracy: 0.8643\n",
            "Validation Accuracy: 0.8497\n",
            "\n",
            "Epoch 3/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2342, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.2006, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.0439, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.4229, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.3927, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.1681, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.0819, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.2977, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.1624, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.2002, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.3219, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.2396, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.2039, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.3104, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.3500, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.3870, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.3262, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1277, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.1914, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.4972, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.4324, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.1481, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.3253, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.5974, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.3036, Batch Acc: 0.8750\n",
            "Epoch 3 Summary - Loss: 80.8910, Train Accuracy: 0.8815\n",
            "Validation Accuracy: 0.8892\n",
            "\n",
            "Epoch 4/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4077, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1337, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.3628, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.1933, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.0825, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.4076, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.4070, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.3432, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.4123, Batch Acc: 0.6875\n",
            "[Batch 100/251] Loss: 0.3758, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.2142, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.1554, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.4406, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.1703, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.0958, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.2087, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.1191, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.7024, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.1874, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.3930, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.2280, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.0929, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.3991, Batch Acc: 0.7500\n",
            "[Batch 240/251] Loss: 0.1122, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.2929, Batch Acc: 0.9375\n",
            "Epoch 4 Summary - Loss: 75.2148, Train Accuracy: 0.8937\n",
            "Validation Accuracy: 0.8892\n",
            "\n",
            "Epoch 5/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1641, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.4952, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.1145, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.2663, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.5383, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.0991, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.0539, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.0797, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.2503, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.1734, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.5909, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.1389, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.1100, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.3060, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.2793, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.7062, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.6324, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.2259, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.1459, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.1187, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.1786, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.2604, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3766, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.2928, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.5333, Batch Acc: 0.8125\n",
            "Epoch 5 Summary - Loss: 67.1462, Train Accuracy: 0.9064\n",
            "Validation Accuracy: 0.8971\n",
            "\n",
            "Epoch 6/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2105, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.1186, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.2030, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.1212, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.3697, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.2834, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.4930, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.1507, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.0305, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.2047, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.2189, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1014, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.0963, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.1027, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.3849, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.1782, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.8266, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.6030, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.1114, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.1688, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.2210, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.4259, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3099, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.2719, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.4371, Batch Acc: 0.8750\n",
            "Epoch 6 Summary - Loss: 62.5657, Train Accuracy: 0.9107\n",
            "Validation Accuracy: 0.8942\n",
            "\n",
            "Epoch 7/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2249, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.4490, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.1665, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.3815, Batch Acc: 0.7500\n",
            "[Batch 50/251] Loss: 0.2449, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.0889, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.1113, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.2698, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.3367, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.1753, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.3241, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1399, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.0539, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.5374, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.5770, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.1585, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.1645, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.0626, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.2379, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1675, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.0382, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.4938, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.1529, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.1745, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.3751, Batch Acc: 0.8750\n",
            "Epoch 7 Summary - Loss: 58.2692, Train Accuracy: 0.9189\n",
            "Validation Accuracy: 0.8991\n",
            "\n",
            "Epoch 8/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1358, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.4106, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.2504, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.3491, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.2706, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.0593, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.2108, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.2592, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.2262, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.5875, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.1024, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.4560, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.0268, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.2246, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.1919, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.1180, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.2274, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.1543, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3363, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.0900, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.1899, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.3554, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3643, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.0883, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.2368, Batch Acc: 0.9375\n",
            "Epoch 8 Summary - Loss: 60.8892, Train Accuracy: 0.9109\n",
            "Validation Accuracy: 0.9021\n",
            "\n",
            "Epoch 9/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2453, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.1808, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.2462, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.2021, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.3640, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.2792, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.3272, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1039, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.0609, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.3901, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.1157, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.4120, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.2949, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.2378, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.0481, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.1495, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.3014, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.3872, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.0690, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.5252, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.4853, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.1754, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.0397, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.1689, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.2019, Batch Acc: 0.8750\n",
            "Epoch 9 Summary - Loss: 56.8074, Train Accuracy: 0.9179\n",
            "Validation Accuracy: 0.8922\n",
            "\n",
            "Epoch 10/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1436, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.0487, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.2190, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.0846, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.0927, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.0832, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.1486, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.1668, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.6008, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.1399, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.2207, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1684, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.2221, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.0963, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.0712, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.1228, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.1589, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.3210, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.2454, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1330, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.1369, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.2142, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.0451, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.2021, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1994, Batch Acc: 1.0000\n",
            "Epoch 10 Summary - Loss: 55.2753, Train Accuracy: 0.9202\n",
            "Validation Accuracy: 0.8922\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}