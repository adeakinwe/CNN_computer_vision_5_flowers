{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeakinwe/computer_vision/blob/main/GoogleNet_PyTorch_5_flowers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekVzSW8Bb_Ze",
        "outputId": "483ed069-e88a-46cc-dc45-8c60b3068e4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import wandb\n",
        "import os"
      ],
      "metadata": {
        "id": "HUjjD6MRYJmr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 0: Initialize wandb\n",
        "# =======================\n",
        "wandb.init(project=\"Inception-5flowers-Classifier\", config={\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"InceptionV1\",\n",
        "    \"pretrained\": True,\n",
        "    \"input_size\": 224\n",
        "})\n",
        "\n",
        "# Shortcut to config values\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "zxVl69yfYLO3",
        "outputId": "435c3b03-828b-4b51-9243-97b37e1de16d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-darkness-1</strong> at: <a href='https://wandb.ai/adeakinwe-richkinwe-io/Inception-flowers/runs/qmpizfqv' target=\"_blank\">https://wandb.ai/adeakinwe-richkinwe-io/Inception-flowers/runs/qmpizfqv</a><br> View project at: <a href='https://wandb.ai/adeakinwe-richkinwe-io/Inception-flowers' target=\"_blank\">https://wandb.ai/adeakinwe-richkinwe-io/Inception-flowers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250713_103404-qmpizfqv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250713_103527-7dr6fe08</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/adeakinwe-richkinwe-io/Inception-5flowers-Classifier/runs/7dr6fe08' target=\"_blank\">noble-plant-1</a></strong> to <a href='https://wandb.ai/adeakinwe-richkinwe-io/Inception-5flowers-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/adeakinwe-richkinwe-io/Inception-5flowers-Classifier' target=\"_blank\">https://wandb.ai/adeakinwe-richkinwe-io/Inception-5flowers-Classifier</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/adeakinwe-richkinwe-io/Inception-5flowers-Classifier/runs/7dr6fe08' target=\"_blank\">https://wandb.ai/adeakinwe-richkinwe-io/Inception-5flowers-Classifier/runs/7dr6fe08</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 1: Data Preparation\n",
        "# =======================\n",
        "\n",
        "# Transforms for training and validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/5flowersdata/flowers/train\"\n",
        "val_dir = \"/content/drive/MyDrive/5flowersdata/flowers/val\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)"
      ],
      "metadata": {
        "id": "Ti4rs4tkYMos"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# STEP 2: Load Pretrained Model\n",
        "# ===========================\n",
        "from torchvision.models import GoogLeNet_Weights\n",
        "\n",
        "\n",
        "# Load pretrained GoogLeNet (Inception v1)\n",
        "model = models.googlenet(weights=GoogLeNet_Weights.DEFAULT)\n",
        "\n",
        "# Replace the final FC layer to match 5 flower classes\n",
        "model.fc = nn.Linear(model.fc.in_features, 5)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze only the final classification layer\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Watch the model's weights and gradients\n",
        "wandb.watch(model, log=\"all\", log_freq=10)"
      ],
      "metadata": {
        "id": "Uu1_CDgvYOEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b67d48a-7436-4f50-fb21-a48adcf5085a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 179MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# STEP 3: Loss & Optimizer\n",
        "# ===================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "id": "mxiVbXfxYPiW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_g-opvZQESck"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            batch_correct = (preds == labels).sum().item()\n",
        "            train_correct += batch_correct\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            # Print every 10 batches\n",
        "            if (i + 1) % 10 == 0:\n",
        "                batch_acc = batch_correct / labels.size(0)\n",
        "                print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
        "\n",
        "        train_acc = train_correct / train_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": running_loss, \"train_accuracy\": train_acc})\n",
        "        print(f\"Epoch {epoch+1} Summary - Loss: {running_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"val_accuracy\": val_acc})\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# Train the model\n",
        "# ===================\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=config.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTczSawNYSrI",
        "outputId": "5b6d15a8-d169-4647-fdf6-2be8eb5a1e3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 1.3598, Batch Acc: 0.5000\n",
            "[Batch 20/251] Loss: 1.3195, Batch Acc: 0.3750\n",
            "[Batch 30/251] Loss: 1.2941, Batch Acc: 0.5000\n",
            "[Batch 40/251] Loss: 1.0769, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.9229, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 1.0952, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 1.1559, Batch Acc: 0.6250\n",
            "[Batch 80/251] Loss: 0.8467, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.7729, Batch Acc: 0.7500\n",
            "[Batch 100/251] Loss: 0.8387, Batch Acc: 0.6875\n",
            "[Batch 110/251] Loss: 1.1415, Batch Acc: 0.6250\n",
            "[Batch 120/251] Loss: 0.8114, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.7742, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.6718, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.7895, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.5369, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.5813, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 1.0386, Batch Acc: 0.5000\n",
            "[Batch 190/251] Loss: 1.0663, Batch Acc: 0.5000\n",
            "[Batch 200/251] Loss: 0.8627, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.8790, Batch Acc: 0.6875\n",
            "[Batch 220/251] Loss: 0.5955, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.5040, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.5052, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.6833, Batch Acc: 0.8125\n",
            "Epoch 1 Summary - Loss: 220.6173, Train Accuracy: 0.7223\n",
            "Validation Accuracy: 0.8338\n",
            "\n",
            "Epoch 2/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.7026, Batch Acc: 0.6250\n",
            "[Batch 20/251] Loss: 0.3764, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.6342, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.4969, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.6404, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.6453, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.5029, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.5753, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.4279, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.6083, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.4912, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.5791, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.6317, Batch Acc: 0.6875\n",
            "[Batch 140/251] Loss: 0.5099, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.3800, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.6608, Batch Acc: 0.6875\n",
            "[Batch 170/251] Loss: 0.5938, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.4368, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.8606, Batch Acc: 0.4375\n",
            "[Batch 200/251] Loss: 0.7391, Batch Acc: 0.6250\n",
            "[Batch 210/251] Loss: 0.3944, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.9736, Batch Acc: 0.5625\n",
            "[Batch 230/251] Loss: 0.9645, Batch Acc: 0.5625\n",
            "[Batch 240/251] Loss: 0.5564, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.4949, Batch Acc: 0.8125\n",
            "Epoch 2 Summary - Loss: 144.7501, Train Accuracy: 0.8046\n",
            "Validation Accuracy: 0.8714\n",
            "\n",
            "Epoch 3/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5336, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.3403, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.5887, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.5813, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.4251, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.3267, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.3479, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.6756, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.3336, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.2578, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.7867, Batch Acc: 0.6875\n",
            "[Batch 120/251] Loss: 0.2105, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.6849, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.6311, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.5002, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.4040, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.3427, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.6357, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.4964, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.8080, Batch Acc: 0.5625\n",
            "[Batch 210/251] Loss: 0.8137, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.7748, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.3426, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.3507, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.4656, Batch Acc: 0.8750\n",
            "Epoch 3 Summary - Loss: 134.5990, Train Accuracy: 0.8121\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 4/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.7992, Batch Acc: 0.6250\n",
            "[Batch 20/251] Loss: 1.1162, Batch Acc: 0.6250\n",
            "[Batch 30/251] Loss: 0.4899, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.2621, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2263, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.6926, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.9466, Batch Acc: 0.6875\n",
            "[Batch 80/251] Loss: 0.3305, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.5018, Batch Acc: 0.7500\n",
            "[Batch 100/251] Loss: 0.5211, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.9198, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.5623, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.3053, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.3992, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.4685, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.4229, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.4231, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.4056, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.9657, Batch Acc: 0.6875\n",
            "[Batch 200/251] Loss: 0.5721, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.6527, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.5543, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.5177, Batch Acc: 0.6875\n",
            "[Batch 240/251] Loss: 0.4418, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.5981, Batch Acc: 0.7500\n",
            "Epoch 4 Summary - Loss: 124.5602, Train Accuracy: 0.8268\n",
            "Validation Accuracy: 0.8536\n",
            "\n",
            "Epoch 5/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3880, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.5800, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.5752, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.2760, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.2490, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.2955, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.8210, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.9881, Batch Acc: 0.6250\n",
            "[Batch 90/251] Loss: 0.3619, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.5369, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.3856, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.3926, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.4012, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.8540, Batch Acc: 0.5625\n",
            "[Batch 150/251] Loss: 0.5827, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.6790, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.3044, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.6238, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.6430, Batch Acc: 0.7500\n",
            "[Batch 200/251] Loss: 0.6069, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.6180, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.5101, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.1875, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.5730, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.3883, Batch Acc: 0.8125\n",
            "Epoch 5 Summary - Loss: 118.5371, Train Accuracy: 0.8333\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Epoch 6/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4398, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.5946, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.3046, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.2887, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.3662, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.1525, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.2143, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.4687, Batch Acc: 0.6875\n",
            "[Batch 90/251] Loss: 0.4890, Batch Acc: 0.6875\n",
            "[Batch 100/251] Loss: 1.0234, Batch Acc: 0.5625\n",
            "[Batch 110/251] Loss: 0.6301, Batch Acc: 0.6250\n",
            "[Batch 120/251] Loss: 0.2606, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.4675, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4452, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.4681, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.4101, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.5776, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.5412, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.3001, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.3170, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.4441, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.5691, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3451, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.8185, Batch Acc: 0.6875\n",
            "[Batch 250/251] Loss: 0.4181, Batch Acc: 0.8750\n",
            "Epoch 6 Summary - Loss: 111.8750, Train Accuracy: 0.8361\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 7/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4132, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.4164, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.1804, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.6532, Batch Acc: 0.6875\n",
            "[Batch 50/251] Loss: 0.4636, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.5315, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.4826, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.3328, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.1968, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.4956, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.2046, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.3002, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.9276, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.6999, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.3357, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.2173, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.3031, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.5604, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.8539, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.5040, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.5152, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.6291, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.2223, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.4959, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.3619, Batch Acc: 0.8750\n",
            "Epoch 7 Summary - Loss: 106.8203, Train Accuracy: 0.8505\n",
            "Validation Accuracy: 0.8684\n",
            "\n",
            "Epoch 8/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4976, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.3638, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.6552, Batch Acc: 0.6875\n",
            "[Batch 40/251] Loss: 0.6045, Batch Acc: 0.7500\n",
            "[Batch 50/251] Loss: 0.5782, Batch Acc: 0.6875\n",
            "[Batch 60/251] Loss: 0.3580, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.3023, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.2481, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.7454, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.4862, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.1671, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.2890, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.2212, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.8186, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.1827, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.4897, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.1553, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.2578, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3603, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.5948, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.8646, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.4507, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.4795, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.5216, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.4364, Batch Acc: 0.9375\n",
            "Epoch 8 Summary - Loss: 112.6798, Train Accuracy: 0.8391\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Epoch 9/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5515, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.2503, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.3844, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.5252, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.1909, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.5003, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.2633, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.3950, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.4748, Batch Acc: 0.6875\n",
            "[Batch 100/251] Loss: 0.2797, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.4398, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.3244, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.2826, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4590, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.3211, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.7064, Batch Acc: 0.6875\n",
            "[Batch 170/251] Loss: 0.5340, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.3297, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.3047, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2497, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.4881, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.1953, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.3213, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.6376, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.6810, Batch Acc: 0.7500\n",
            "Epoch 9 Summary - Loss: 102.9089, Train Accuracy: 0.8515\n",
            "Validation Accuracy: 0.8912\n",
            "\n",
            "Epoch 10/10\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3224, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.2136, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.1729, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.3718, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.4644, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.5815, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.4254, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.2258, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.6895, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.1321, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.3058, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.3701, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.2225, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.1866, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.6758, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.4532, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.8080, Batch Acc: 0.6875\n",
            "[Batch 180/251] Loss: 0.2829, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.4544, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2539, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.3438, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.2482, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.2245, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.5482, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.5113, Batch Acc: 0.8125\n",
            "Epoch 10 Summary - Loss: 105.3904, Train Accuracy: 0.8473\n",
            "Validation Accuracy: 0.8882\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}